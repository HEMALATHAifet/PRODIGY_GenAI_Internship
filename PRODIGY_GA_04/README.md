# Task-04: Image-to-Image Translation with cGAN

> Implement an image-to-image translation model using a conditional generative adversarial network (cGAN) called **Pix2Pix**.

---

## 🔍 Domain

**Computer Vision** – specifically, Image-to-Image Translation using Generative Adversarial Networks (GANs).

---

## 🎯 What is Image-to-Image Translation?

Image-to-image translation is a task in computer vision where an input image is transformed into a corresponding output image. The transformation depends on the task — such as converting:

- Sketches → Realistic images
- Daytime photos → Nighttime photos
- Grayscale → Color images

In our case: **Edge Sketch → Realistic Face Image**

---

## 🤖 What is cGAN?

A **Conditional GAN (cGAN)** is a type of Generative Adversarial Network where both the **generator** and **discriminator** receive additional **condition information**, such as class labels or input images.

- **Generator (G)** tries to produce realistic images from input conditions (e.g., edge sketches).
- **Discriminator (D)** tries to distinguish between real images and those generated by G, conditioned on the input.

---

## 🖼️ What is Pix2Pix?

**Pix2Pix** is a type of cGAN architecture designed for image-to-image translation. It consists of:

- A **U-Net based Generator** – for capturing global and local features from the input.
- A **PatchGAN Discriminator** – evaluates the realism of local image patches instead of the whole image.

---

## 🎯 Purpose of This Project

The goal of this project is to:
- Implement a Pix2Pix model using TensorFlow.
- Train it on a dataset containing **edge-to-face** image pairs.
- Provide a Gradio interface for real-time edge-to-face generation.

---

## 📁 Dataset

Dataset used: **Edge2Face**

- Each training image is a side-by-side (512x256) image where:
  - Left side: Edge sketch
  - Right side: Corresponding face

Folder structure:

```

edge2face/
├── train/
│   ├── img1.png
│   ├── img2.png
│   └── ...

````

---

## 📜 Code Explanation

### ✅ Preprocessing
- Load `.png` image pairs.
- Split image into input (edges) and target (faces).
- Resize to 256x256 and normalize to [-1, 1].

### ✅ Model
- `pix2pix.unet_generator()` for Generator.
- `pix2pix.discriminator()` for Discriminator.
- Loss functions: Binary Crossentropy + L1 Loss (for generator).

### ✅ Training
- Run training loop for `EPOCHS` (default: 3 for demo).
- Save intermediate results for visualization (optional).

### ✅ Gradio App
- Upload an edge image (256x256).
- Generator predicts realistic face.
- Result shown directly in the browser.

---

## 🚀 How to Run

```bash
# Upload edge2face.zip in Google Colab
# Run the notebook

# Start the app
demo.launch()
````

---

## 🔮 Future Enhancements

* 🔁 **Train for more epochs**: Improve image quality by increasing `EPOCHS`.
* 💾 **Model checkpointing**: Save and load trained models for reuse.
* 🎨 **Add new domains**: Apply Pix2Pix to other tasks like:

  * Maps → Satellite images
  * BW → Color images
* 🧪 **Use Augmentation**: Improve generalization with image augmentation.
* 📈 **Metrics**: Add FID/SSIM/PSNR to measure performance.

---

## 📚 References

1. [Pix2Pix Paper](https://arxiv.org/abs/1611.07004)
2. [TensorFlow Pix2Pix Tutorial](https://www.tensorflow.org/tutorials/generative/pix2pix)
3. [GitHub - TensorFlow Examples](https://github.com/tensorflow/examples)

---

